+++
# Date this page was created.
date = "2018-04-27"

# Project title.
title = "Optimization with Optimistic Descent Algorithms"

# Project summary to display on homepage.
summary = "Optimistic descent methods for convex and saddle point objective functions"

# Optional image to display on homepage (relative to `static/img/` folder).
image_preview = ""

# Tags: can be used for filtering projects.
# Example: `tags = ["machine-learning", "deep-learning"]`
tags = []

# Optional external URL for project (replaces project detail page).
external_link = ""

# Does the project detail page use math formatting?
math = false

# Optional featured image (relative to `static/img/` folder).
[header]
image = ""
caption = ""
+++

This work focuses on the review and applications of extra gradient methods popularly referred to as optimistic descent algorithms. First we introduce the concept optimistic mirror descent (OMD) and provide intuition as well as motivation behind this concept. Second we evaluate the performance of optimistic methods across a host of applications. In particular we compare the performance of OMD in convex, non-convex and saddle point problem settings. Lastly we observe that OMD has certain fundamental limitations and only outperforms the existing methods under certain specific considerations.

This project was undertaken as part of Large Scale Optimization course during Jan-May 2019. Further details about the project and results are available {{% staticref "pdf/Report_LSO2.pdf" "newtab" %}}here{{% /staticref %}}.

